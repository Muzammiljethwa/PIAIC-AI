{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Following Dataset and use five text(use randomly generated 5 numbers and extract the data of the generated numbers from the dataset) to test the assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Unique Weird Orientalia from the 1930's: Exotic tales of the Orient from the 1930's. 'Dr Shen Fu', a Weird Tales magazine reprint, is about the elixir of life that grants immortality at a price. If you're tired of modern authors who all sound alike, this is the antidote for you. Owen's palette is loaded with splashes of Chinese and Japanese colours. Marvelous.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Wonderful, baffling mix: This has just about everything. The rabbi, of course, is learned and devout, even if his schooling causes some nervousness. The cat becomes fantastical when some mystery grants him human speech. The visit abroad captures all the distrust of the alien that the most insular of small-towners can summon. And, around all of that, the monotonous drama of life unfolds: loves, losses, and changes that no one knows how to ride out.Comics have moved well past the BamPow genre into many other idioms - this might border on chick lit, but much less than some. If reality and fantasy remain negotiable for you, with deep family devotions and deeper (or shallower) personal ones, the ragged lines of this comic might touch you.-- wiredweird\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"No, not really: The title of this CD is misleading in an amazing number of ways. None of the songs are in any way related to rock music, nor, with one exception, are any of the artists. Even calling it a compilation of 60's folk music is a stretch. There are a handful of selections by artists from the early 60's folk scene, but planted among them are songs by a couple of 'folk-lite' bands (in the Kingston Trio/PP&M vein), two pop groups, and for some reason, a song by Traffic. I would be fascinated to learn what, in someone's mind, tied these together. Unless you are looking for a specific song, you can skip this one. Amazon sells dozens of other fine collections which better define both \"\"60's folk\" \"and \"\"the roots of rock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = \"Colin Farrells performance is the only interesting thing here: Weak script, the actors do an acceptable job of trying to breathe some life and depth into it but they still dont manage to salvage the film. Theres an air of disdain and snottiness in the script-writers approach to the characters that makes them feel unemotional, plastic. Colin Farrell's role is brief and he plays the worst person in the film but he manages to inject the role with humanity. You believe he is a messed up sociopath but at the same time feel sorry for him on some level without this emotion being forced out of you.Which is good but stands in contrast to most of the rest of the film where the emotions feel forced,over the top and unbelievable. In short theres very few parts of the film where you believe this is anything but people reading lines and playing parts.Colin Farrell saved the film from the scrapheap but beyond his small part theres not much to recommend here unfortunately.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = \"Plays with it all the time!: I got this for my son when he was about a year old. He hasn't put it down since. The first thing he does is get out his clubs in the morning and now at 17 months he takes the cart every where. This is very durable and keeps him happy. I highly recommend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Unique Weird Orientalia from the 1930's: Exotic tales of the Orient from the 1930's. 'Dr Shen Fu', a Weird Tales magazine reprint, is about the elixir of life that grants immortality at a price. If you're tired of modern authors who all sound alike, this is the antidote for you. Owen's palette is loaded with splashes of Chinese and Japanese colours. Marvelous.\",\n",
       " 'Wonderful, baffling mix: This has just about everything. The rabbi, of course, is learned and devout, even if his schooling causes some nervousness. The cat becomes fantastical when some mystery grants him human speech. The visit abroad captures all the distrust of the alien that the most insular of small-towners can summon. And, around all of that, the monotonous drama of life unfolds: loves, losses, and changes that no one knows how to ride out.Comics have moved well past the BamPow genre into many other idioms - this might border on chick lit, but much less than some. If reality and fantasy remain negotiable for you, with deep family devotions and deeper (or shallower) personal ones, the ragged lines of this comic might touch you.-- wiredweird',\n",
       " \"No, not really: The title of this CD is misleading in an amazing number of ways. None of the songs are in any way related to rock music, nor, with one exception, are any of the artists. Even calling it a compilation of 60's folk music is a stretch. There are a handful of selections by artists from the early 60's folk scene, but planted among them are songs by a couple of 'folk-lite' bands (in the Kingston Trio/PP&M vein), two pop groups, and for some reason, a song by Traffic. I would be fascinated to learn what, in someone's mind, tied these together. Unless you are looking for a specific song, you can skip this one. Amazon sells dozens of other fine collections which better define both 60's folkand the roots of rock\",\n",
       " \"Colin Farrells performance is the only interesting thing here: Weak script, the actors do an acceptable job of trying to breathe some life and depth into it but they still dont manage to salvage the film. Theres an air of disdain and snottiness in the script-writers approach to the characters that makes them feel unemotional, plastic. Colin Farrell's role is brief and he plays the worst person in the film but he manages to inject the role with humanity. You believe he is a messed up sociopath but at the same time feel sorry for him on some level without this emotion being forced out of you.Which is good but stands in contrast to most of the rest of the film where the emotions feel forced,over the top and unbelievable. In short theres very few parts of the film where you believe this is anything but people reading lines and playing parts.Colin Farrell saved the film from the scrapheap but beyond his small part theres not much to recommend here unfortunately.\",\n",
       " \"Plays with it all the time!: I got this for my son when he was about a year old. He hasn't put it down since. The first thing he does is get out his clubs in the morning and now at 17 months he takes the cart every where. This is very durable and keeps him happy. I highly recommend.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [text1,text2,text3,text4,text5]\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)  One-hot coding of the following data using book codes (naive codes) of listing 6.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unique': 1, 'Weird': 2, 'Orientalia': 3, 'from': 4, 'the': 5, \"1930's:\": 6, 'Exotic': 7, 'tales': 8, 'of': 9, 'Orient': 10, \"1930's.\": 11, \"'Dr\": 12, 'Shen': 13, \"Fu',\": 14, 'a': 15, 'Tales': 16, 'magazine': 17, 'reprint,': 18, 'is': 19, 'about': 20, 'elixir': 21, 'life': 22, 'that': 23, 'grants': 24, 'immortality': 25, 'at': 26, 'price.': 27, 'If': 28, \"you're\": 29, 'tired': 30, 'modern': 31, 'authors': 32, 'who': 33, 'all': 34, 'sound': 35, 'alike,': 36, 'this': 37, 'antidote': 38, 'for': 39, 'you.': 40, \"Owen's\": 41, 'palette': 42, 'loaded': 43, 'with': 44, 'splashes': 45, 'Chinese': 46, 'and': 47, 'Japanese': 48, 'colours.': 49, 'Marvelous.': 50, 'Wonderful,': 51, 'baffling': 52, 'mix:': 53, 'This': 54, 'has': 55, 'just': 56, 'everything.': 57, 'The': 58, 'rabbi,': 59, 'course,': 60, 'learned': 61, 'devout,': 62, 'even': 63, 'if': 64, 'his': 65, 'schooling': 66, 'causes': 67, 'some': 68, 'nervousness.': 69, 'cat': 70, 'becomes': 71, 'fantastical': 72, 'when': 73, 'mystery': 74, 'him': 75, 'human': 76, 'speech.': 77, 'visit': 78, 'abroad': 79, 'captures': 80, 'distrust': 81, 'alien': 82, 'most': 83, 'insular': 84, 'small-towners': 85, 'can': 86, 'summon.': 87, 'And,': 88, 'around': 89, 'that,': 90, 'monotonous': 91, 'drama': 92, 'unfolds:': 93, 'loves,': 94, 'losses,': 95, 'changes': 96, 'no': 97, 'one': 98, 'knows': 99, 'how': 100, 'to': 101, 'ride': 102, 'out.Comics': 103, 'have': 104, 'moved': 105, 'well': 106, 'past': 107, 'BamPow': 108, 'genre': 109, 'into': 110, 'many': 111, 'other': 112, 'idioms': 113, '-': 114, 'might': 115, 'border': 116, 'on': 117, 'chick': 118, 'lit,': 119, 'but': 120, 'much': 121, 'less': 122, 'than': 123, 'some.': 124, 'reality': 125, 'fantasy': 126, 'remain': 127, 'negotiable': 128, 'you,': 129, 'deep': 130, 'family': 131, 'devotions': 132, 'deeper': 133, '(or': 134, 'shallower)': 135, 'personal': 136, 'ones,': 137, 'ragged': 138, 'lines': 139, 'comic': 140, 'touch': 141, 'you.--': 142, 'wiredweird': 143, 'No,': 144, 'not': 145, 'really:': 146, 'title': 147, 'CD': 148, 'misleading': 149, 'in': 150, 'an': 151, 'amazing': 152, 'number': 153, 'ways.': 154, 'None': 155, 'songs': 156, 'are': 157, 'any': 158, 'way': 159, 'related': 160, 'rock': 161, 'music,': 162, 'nor,': 163, 'exception,': 164, 'artists.': 165, 'Even': 166, 'calling': 167, 'it': 168, 'compilation': 169, \"60's\": 170, 'folk': 171, 'music': 172, 'stretch.': 173, 'There': 174, 'handful': 175, 'selections': 176, 'by': 177, 'artists': 178, 'early': 179, 'scene,': 180, 'planted': 181, 'among': 182, 'them': 183, 'couple': 184, \"'folk-lite'\": 185, 'bands': 186, '(in': 187, 'Kingston': 188, 'Trio/PP&M': 189, 'vein),': 190, 'two': 191, 'pop': 192, 'groups,': 193, 'reason,': 194, 'song': 195, 'Traffic.': 196, 'I': 197, 'would': 198, 'be': 199, 'fascinated': 200, 'learn': 201, 'what,': 202, \"someone's\": 203, 'mind,': 204, 'tied': 205, 'these': 206, 'together.': 207, 'Unless': 208, 'you': 209, 'looking': 210, 'specific': 211, 'song,': 212, 'skip': 213, 'one.': 214, 'Amazon': 215, 'sells': 216, 'dozens': 217, 'fine': 218, 'collections': 219, 'which': 220, 'better': 221, 'define': 222, 'both': 223, 'folkand': 224, 'roots': 225, 'Colin': 226, 'Farrells': 227, 'performance': 228, 'only': 229, 'interesting': 230, 'thing': 231, 'here:': 232, 'Weak': 233, 'script,': 234, 'actors': 235, 'do': 236, 'acceptable': 237, 'job': 238, 'trying': 239, 'breathe': 240, 'depth': 241, 'they': 242, 'still': 243, 'dont': 244, 'manage': 245, 'salvage': 246, 'film.': 247, 'Theres': 248, 'air': 249, 'disdain': 250, 'snottiness': 251, 'script-writers': 252, 'approach': 253, 'characters': 254, 'makes': 255, 'feel': 256, 'unemotional,': 257, 'plastic.': 258, \"Farrell's\": 259, 'role': 260, 'brief': 261, 'he': 262, 'plays': 263, 'worst': 264, 'person': 265, 'film': 266, 'manages': 267, 'inject': 268, 'humanity.': 269, 'You': 270, 'believe': 271, 'messed': 272, 'up': 273, 'sociopath': 274, 'same': 275, 'time': 276, 'sorry': 277, 'level': 278, 'without': 279, 'emotion': 280, 'being': 281, 'forced': 282, 'out': 283, 'you.Which': 284, 'good': 285, 'stands': 286, 'contrast': 287, 'rest': 288, 'where': 289, 'emotions': 290, 'forced,over': 291, 'top': 292, 'unbelievable.': 293, 'In': 294, 'short': 295, 'theres': 296, 'very': 297, 'few': 298, 'parts': 299, 'anything': 300, 'people': 301, 'reading': 302, 'playing': 303, 'parts.Colin': 304, 'Farrell': 305, 'saved': 306, 'scrapheap': 307, 'beyond': 308, 'small': 309, 'part': 310, 'recommend': 311, 'here': 312, 'unfortunately.': 313, 'Plays': 314, 'time!:': 315, 'got': 316, 'my': 317, 'son': 318, 'was': 319, 'year': 320, 'old.': 321, 'He': 322, \"hasn't\": 323, 'put': 324, 'down': 325, 'since.': 326, 'first': 327, 'does': 328, 'get': 329, 'clubs': 330, 'morning': 331, 'now': 332, '17': 333, 'months': 334, 'takes': 335, 'cart': 336, 'every': 337, 'where.': 338, 'durable': 339, 'keeps': 340, 'happy.': 341, 'highly': 342, 'recommend.': 343}\n"
     ]
    }
   ],
   "source": [
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "print(token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 50, 344)\n",
      "It means 5 sentences, \n",
      "take maximum 50 words from each sentence, \n",
      "And each word can be of anyone from above dictionary\n"
     ]
    }
   ],
   "source": [
    "results = np.zeros(shape=(len(samples),max_length,max(token_index.values()) + 1))\n",
    "shape_of_1st_result = results.shape\n",
    "print(shape_of_1st_result)\n",
    "print(\"It means 5 sentences, \\ntake maximum 50 words from each sentence, \\nAnd each word can be of anyone from above dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function to extract Word Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(one_hot_encoded_list, sentence, word):\n",
    "    print(\"One Hot Encoded word:\",one_hot_encoded_list[sentence][word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoded word: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "find_word(results,4,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Also, provide one-hot coding using Keras built-in function (listing 6.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Tokenizer from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 315 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary after tokenizing:  {'the': 1, 'of': 2, 'and': 3, 'is': 4, 'a': 5, 'this': 6, 'to': 7}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "print(\"dictionary after tokenizing: \",dict(itertools.islice(word_index.items(), 7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of Sentences after Tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74, 37, 75, 18, 1, 38, 76, 39, 2, 1, 77, 18, 1, 38, 78, 79, 80, 5, 37, 39, 81, 82, 4, 21, 1, 83, 2, 22, 14, 40, 84, 23, 5, 85, 24, 86, 87, 2, 88, 89, 90, 19, 91, 92, 6, 4, 1, 93, 12, 9, 94, 95, 4, 96, 15, 97, 2, 98, 3, 99, 100, 101], [102, 103, 104, 6, 105, 106, 21, 107, 1, 108, 2, 109, 4, 110, 3, 111, 41, 24, 25, 112, 113, 13, 114, 1, 115, 116, 117, 42, 13, 118, 40, 26, 119, 120, 1, 121, 122, 123, 19, 1, 124, 2, 1, 125, 14, 1, 43, 126, 2, 44, 127, 45, 128, 3, 129, 19, 2, 14, 1, 130, 131, 2, 22, 132, 133, 134, 3, 135, 14, 46, 27, 136, 137, 7, 138, 28, 139, 140, 141, 142, 143, 1, 144, 145, 47, 146, 48, 147, 6, 49, 148, 50, 149, 150, 10, 51, 151, 152, 13, 24, 153, 3, 154, 155, 156, 12, 9, 15, 157, 158, 159, 3, 160, 161, 162, 163, 164, 1, 165, 52, 2, 6, 166, 49, 167, 9, 168], [46, 53, 169, 1, 170, 2, 6, 171, 4, 172, 8, 29, 173, 174, 2, 175, 176, 2, 1, 54, 16, 8, 55, 177, 178, 7, 56, 57, 179, 15, 27, 180, 16, 55, 2, 1, 58, 41, 181, 20, 5, 182, 2, 30, 59, 57, 4, 5, 183, 184, 16, 5, 185, 2, 186, 31, 58, 18, 1, 187, 30, 59, 188, 10, 189, 190, 60, 16, 54, 31, 5, 191, 2, 192, 193, 194, 8, 1, 195, 196, 197, 198, 199, 200, 201, 202, 3, 12, 13, 203, 5, 61, 31, 204, 32, 205, 206, 207, 7, 208, 209, 8, 210, 211, 212, 213, 214, 215, 9, 16, 216, 12, 5, 217, 61, 9, 45, 218, 6, 27, 219, 220, 221, 2, 48, 222, 223, 62, 224, 225, 226, 30, 227, 1, 228, 2, 56], [33, 229, 230, 4, 1, 231, 232, 63, 64, 233, 65, 1, 234, 235, 29, 236, 237, 2, 238, 7, 239, 13, 22, 3, 240, 47, 20, 10, 241, 242, 243, 244, 7, 245, 1, 17, 34, 29, 246, 2, 247, 3, 248, 8, 1, 65, 249, 250, 7, 1, 251, 14, 252, 60, 35, 253, 254, 33, 255, 66, 4, 256, 3, 11, 67, 1, 257, 258, 8, 1, 17, 10, 11, 259, 7, 260, 1, 66, 15, 261, 9, 68, 11, 4, 5, 262, 263, 264, 10, 23, 1, 265, 69, 35, 266, 12, 26, 50, 13, 267, 268, 6, 269, 270, 70, 28, 2, 9, 62, 4, 271, 10, 272, 8, 273, 7, 43, 2, 1, 274, 2, 1, 17, 36, 1, 275, 35, 70, 276, 1, 277, 3, 278, 8, 279, 34, 71, 280, 72, 2, 1, 17, 36, 9, 68, 6, 4, 281, 10, 282, 283, 52, 3, 284, 72, 33, 285, 286, 1, 17, 18, 1, 287, 10, 288, 25, 44, 289, 34, 53, 51, 7, 73, 64, 290], [67, 15, 20, 19, 1, 69, 32, 291, 6, 12, 292, 293, 42, 11, 294, 21, 5, 295, 296, 11, 297, 298, 20, 299, 300, 1, 301, 63, 11, 302, 4, 303, 28, 25, 304, 8, 1, 305, 3, 306, 23, 307, 308, 11, 309, 1, 310, 311, 36, 6, 4, 71, 312, 3, 313, 26, 314, 32, 315, 73]]"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "print(sequences ,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n",
      "It means 5 sentences, \n",
      "Each Sentence has maximum 1000 words from above tokens\n",
      "\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "shape_of_2nd_result = one_hot_results.shape\n",
    "print(shape_of_2nd_result)\n",
    "print(\"It means 5 sentences, \\nEach Sentence has maximum 1000 words from above tokens\\n\")\n",
    "print(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function to extract Sentence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sen(one_hot_encoded_list, sentence):\n",
    "    print(\"One Hot Encoded sentence:\",one_hot_encoded_list[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoded sentence: [0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "find_sen(one_hot_results,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) provide one-hot coding with hashing (listing 6.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding with Hashing trick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 1000\n",
    "max_length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Marix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10, 1000)\n",
      "It means 5 sentences, \n",
      "take maximum 10 words from each sentence, \n",
      "And each word can be of anyone from above 1000 words dictionary\n"
     ]
    }
   ],
   "source": [
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "shape_of_3rd_result = results.shape\n",
    "print(shape_of_3rd_result)\n",
    "print(\"It means 5 sentences, \\ntake maximum 10 words from each sentence, \\nAnd each word can be of anyone from above 1000 words dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "key,value=[],[]\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        key.append(j)\n",
    "        value.append(word)\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values In Hashing Tecnique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:Unique, 1:Weird, 2:Orientalia, 3:from, 4:the, 5:1930's:, 6:Exotic, 7:tales, 8:of, 9:the, 0:Wonderful,, 1:baffling, 2:mix:, 3:This, 4:has, 5:just, 6:about, 7:everything., 8:The, 9:rabbi,, 0:No,, 1:not, 2:really:, 3:The, 4:title, 5:of, 6:this, 7:CD, 8:is, 9:misleading, 0:Colin, 1:Farrells, 2:performance, 3:is, 4:the, 5:only, 6:interesting, 7:thing, 8:here:, 9:Weak, 0:Plays, 1:with, 2:it, 3:all, 4:the, 5:time!:, 6:I, 7:got, 8:this, 9:for, "
     ]
    }
   ],
   "source": [
    "for l in range(len(value)):\n",
    "    print(f\"{key[l]}:{value[l]}\",end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function to extract Word Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(one_hot_encoded_list, sentence, word):\n",
    "    print(\"One Hot Encoded word:\",one_hot_encoded_list[sentence][word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoded word: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "find_word(results,4,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Validate the how much they are similar. Try to maximize the similarity (ideally should be 100% the same). Specify the reason if both are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Shape of 1st, 2nd and 3rd Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Technique: (5, 50, 344)\n",
      "In this Technique the sentence is limited to 50 words and each word can be of any unique word from the given data.\n",
      "\n",
      "Keras Tokenizer: (5, 1000)\n",
      "In this techinque, Each sentence is limited to 1000 words and keras encoded whole sentence at a time and saved the data in 1 list.\n",
      "\n",
      "Hashing Technique: (5, 10, 1000)\n",
      "In this technique, Each sentence is limited to 10 words and each word can be of any one word from 1000 unique words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Normal Technique:\",shape_of_1st_result)\n",
    "print(\"In this Technique the sentence is limited to 50 words and each word can be of any unique \"\n",
    "      \"word from the given data.\\n\")\n",
    "print(\"Keras Tokenizer:\",shape_of_2nd_result)\n",
    "print(\"In this techinque, Each sentence is limited to 1000 words and keras encoded whole sentence\"\n",
    "      \" at a time and saved the data in 1 list.\\n\")\n",
    "print(\"Hashing Technique:\",shape_of_3rd_result)\n",
    "print(\"In this technique, Each sentence is limited to 10 words and each word can be of any \"\n",
    "     \"one word from 1000 unique words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Tokenizes Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1)In this Technique each word is uniquely tochnized.\n",
      "\n",
      " Normal Technique: {'Unique': 1, 'Weird': 2, 'Orientalia': 3, 'from': 4, 'the': 5, \"1930's:\": 6, 'Exotic': 7, 'tales': 8, 'of': 9} \n",
      "\n",
      "2) In this Technique most commonly word is tokenized first.\n",
      "\n",
      " Keras Tokenizer: {'the': 1, 'of': 2, 'and': 3, 'is': 4, 'a': 5, 'this': 6, 'to': 7, 'in': 8, 'you': 9} \n",
      "\n",
      "3) In this Technique each word is uniquely tochnized but it tokenized according to each sentence, not all combined.\n",
      "\n",
      " Hashing Technique: \n",
      "0:Unique, 1:Weird, 2:Orientalia, 3:from, 4:the, 5:1930's:, 6:Exotic, 7:tales, 8:of, 9:the, 0:Wonderful,, 1:baffling, 2:mix:, 3:This, 4:has, 5:just, 6:about, 7:everything., 8:The, 9:rabbi,, 0:No,, 1:not, 2:really:, 3:The, 4:title, 5:of, 6:this, 7:CD, 8:is, 9:misleading, 0:Colin, 1:Farrells, 2:performance, 3:is, 4:the, 5:only, 6:interesting, 7:thing, 8:here:, 9:Weak, 0:Plays, 1:with, 2:it, 3:all, 4:the, 5:time!:, 6:I, 7:got, 8:this, 9:for, "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "for i in range(10):\n",
    "    dict_1 = dict(itertools.islice(token_index.items(), i))\n",
    "    dict_2 = dict(itertools.islice(word_index.items(), i))\n",
    "print(\"1)In this Technique each word is uniquely tochnized.\\n\")\n",
    "print(\" Normal Technique:\",dict_1,'\\n')\n",
    "print(\"2) In this Technique most commonly word is tokenized first.\\n\")\n",
    "print(\" Keras Tokenizer:\",dict_2,'\\n')\n",
    "print(\"3) In this Technique each word is uniquely tochnized but it tokenized according \"\n",
    "     \"to each sentence, not all combined.\\n\")\n",
    "print(\" Hashing Technique: \")\n",
    "for l in range(len(value)):\n",
    "    print(f\"{key[l]}:{value[l]}\",end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SO, According to above Discussion Every Technique is different from each other and also has different results. \n",
    "# And Simple One-hot encoding and Hashing Technique are more similar, the only difference between them is the technique of tokenizing which also affects on result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
